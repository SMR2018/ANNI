---
title: "ANNI in Tensorflow: pretrained Model LSTM mit langjährigen Wetterdaten"
author: "Samantha Rubo"
date: "2023-05-03"
---


```{r, message=FALSE}
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
library(ggplot2)
library(data.table)
library(purrr)
```


### Input-Tabelle laden (aus A3_Tabelle_erstellen.RMD)
```{r}
rm(list = ls())
## AMBAV-Daten:
file_list <- list.files("../../DWD_database_ANNI2/data/derived_data/DWD_data/",
                        pattern = "alle_stationsdaten_zusammen", full.names = T)
data1 <- map_df(file_list, ~fread(.)) %>% as_tibble()
str(data1)

## Parameter der exmerimental-Data-Modelle:
data_exp <- data.table::fread(
    "../data/derived_data/input_tabelle_2020_2022_20220808_complete.csv",
    sep = ";", dec = ".") %>% as_tibble()
#Variablen auswählen:
params_fuer_model <- names(data_exp)
params_fuer_model[!params_fuer_model %in% names(data1)] #Diese Variablen fehlen noch bei AMBAV-DWD-Daten
rm(data_exp)

kulturdauer = 14
```

\

```{r}
names_x <- c(#"B0020_nFK_prozent_Vortag",            # Nur MLP, nicht LSTM
    #"B2040_nFK_prozent_Vortag",            # Nur MLP, nicht LSTM
    #"B4060_nFK_prozent_Vortag",            # Nur MLP, nicht LSTM
    #"tage_seit_aussaat", 
    "Tmean_gradC", 
    "Tmean_th9_25_gradC", 
    "Tmin_gradC", 
    "Tmax_gradC", 
    "relLuftfeuchte_mean_prozent", 
    #"relLuftfeuchte_min_prozent", # laut Olden unwichtig
    #"relLuftfeuchte_max_prozent", # s.o.
    "globalstrahlung_Wh_m2", 
    "windgeschwindigkeit_m_s", 
    "tageslicht_h", 
    "tageslicht_h_th9", 
    # "Tmean_gradC_sum",             # s.o.
    # "Tmean_th9_25_gradC_sum",      # s.o.
    # "globalstrahlung_Wh_m2_sum",   # s.o.
    # "niederschlag_mm_sum",         # s.o.
    # "tageslicht_h_sum",            # s.o.
    # "tageslicht_h_th9_sum",        # s.o.
    # "Ton_prozent",
    # "Schluff_prozent",
    # "Sand_prozent",
    # "C_org_prozent",
    # "ibi", 
    # "irmi",
    "wasser_input"
    # "wasser_input_Vortag" ## bei LSTM Tageswert statt Vortageswert
)

names_y <- c(
    "B0020_nFK_prozent", 
    "B2040_nFK_prozent",
    "B4060_nFK_prozent"
)

# data1 <- data0 %>% select(all_of(c(names_y,names_x, "id_all"))) 
```

```{r}
Greznwert_temp <- 9.1
intervalle_pro_satz = 3
n_days_satz <- intervalle_pro_satz * kulturdauer #42

data1 <- 
    data1 %>% 
    group_by(id) %>%
    ########## 1.  Nach id gruppieren, Zeitreihe von 50 Tagen erstellen und fehlende 
    ##########  Meteo-Parameter durch Mittelwerte ergänzen:
    
    #filter(Tmean_gradC > 5) %>% ##dann keine nahtlose zeitreihe mehr
    #filter(between(nFK_1020, 20, 110)) %>% #Wertebereich der obersten Schicht nFK definieren
    mutate(satz_nr = rep(1:ceiling((n()/n_days_satz)), each = n_days_satz, length.out = n())) %>%
    group_by(id, satz_nr) %>%
  #  mutate(tage_seit_aussaat = 1:n()) %>% #select(id, satz_nr, tage_seit_aussaat) %>% slice(1:100)
    filter(n() == n_days_satz) %>% # nur vollständige Zeitreihen filtern
    mutate(teil_intervall = rep(1:intervalle_pro_satz, each = kulturdauer),
           Tmean_th9_25_gradC = ifelse(Tmean_gradC > 25, 25,Tmean_gradC), .after = "Tmean_gradC") %>%
    mutate_at("Tmean_th9_25_gradC", ~ifelse(.<Greznwert_temp,0,.)) %>%
    mutate(tageslicht_h_th9 = ifelse(Tmean_gradC >= Greznwert_temp, tageslicht_h, 0)) %>%
    # mutate(across(c("tageslicht_h", 
    #                 "tageslicht_h_th9",
    #                 "Tmean_gradC", 
    #                 "globalstrahlung_Wh_m2", 
    #                 "Tmean_th9_25_gradC"), 
    #               ~cumsum(ifelse(.<0,0,.)), .names = "{.col}_sum")) %>%
    
    
    #     ########## 2. IBI und IRMI ergänzen:
    #     # ibi und irmi mit Median aus data0 füllen
    # mutate(ibi = 3.32,
    #        irmi = 19.36) %>%
    
    ########## 4. auf 20cm Schritte reduzieren (konsistent mit exp-Model)
    mutate(B0020_nFK_prozent = nFK_1020,
           B2040_nFK_prozent = nFK_3040,
           B4060_nFK_prozent = nFK_5060#,
           # B0020_nFK_prozent_Vortag = nFK_1020_Vortag, ##nur bei MLP
           # B2040_nFK_prozent_Vortag = nFK_3040_Vortag, s.o.
           # B4060_nFK_prozent_Vortag = nFK_5060_Vortag  s.o.
    ) %>%
    
    ########## 5. LSTM: id anfügen:
    group_by(id, satz_nr, teil_intervall) %>%
    mutate(id_all = cur_group_id() )%>%
    group_by(id_all) %>% 
    
    
    ########## 3. unplausible Daten zur Globalstrahlung löschen: (ganzes Intervall)
    mutate(weg = ifelse(globalstrahlung_Wh_m2 < 10000 | 
                            # globalstrahlung_Wh_m2_sum < 300000 |
                            wasser_input_Vortag < 20, 0,1)) %>%
    filter(weg == 0) %>%
    filter(n() == kulturdauer) %>%
    
    ########### 6. Wasser-input korrigieren.
    group_by(id_all) %>%
    # mutate(weg = ifelse(!is.na(wasser_input), 0,1)) %>%
    # filter(weg == 0) %>%
    mutate(across(wasser_input, ~ifelse(is.na(.), 0, .)) ) %>% #entweder auf 0 setzten oder ganzes Intervall entfernen
    
    ungroup() %>%
    select(all_of(c(names_y,names_x, "id_all"))) 
```


```{r}
is.na(data1) %>% any
# gibt es noch NAs im Wasser_input??
which(is.na(data1$wasser_input)) %>% length()
which(!is.na(data1$wasser_input)) %>% length()

```


#ANNI: 
#Matrix für Modell auf DWD-Daten:
## DWD-Daten skalieren

## Skalierungs-Daten einlesen:
```{r}
scaling_data <- fread("../data/derived_data/Models_paper/All_scaling_data_20230328.csv")

names_all <- scaling_data$parameter
m <- scaling_data$m
s <- scaling_data$s

names(m) <- names_all
names(s) <- names_all
```

## DF skalieren
```{r}
df_scaled <- scale(data1[,c(names_y, names_x,"id_all")], 
                   center = m[c(names_y, names_x,NA)], 
                   scale = s[c(names_y, names_x,NA)])

df_scaled[,"id_all"] <- data1$id_all
```

## Trainings- und Test-Daten einteilen:
```{r}
set.seed(123)
ind <- sample(2, nrow(df_scaled), replace = T, prob = c(.7, .3))

#X definieren:
training <- df_scaled[ind==1,names_x]
test <- df_scaled[ind==2, names_x]

#Target definieren:
trainingtarget <- df_scaled[ind==1, names_y] 
testtarget <- df_scaled[ind==2, names_y]
```




# Aufteilung in Trainings- und Test-Datensatz
```{r}
# In Training und Test-Daten einteilen:

ids <- unique(data1$id_all)

## Ids einteilen für gesamten AMBAV-Datensatz
ind <- ceiling(length(ids) * 0.7)
ind_training <- ids[1:ind]
ind_test <- ids[(ind+1):length(ids)]

#Ids nach Ort aufteilen: /// bei X2: LSTM experimental data
# ind <- data0 %>%
#     group_by(Ort) %>% 
#     summarise(nn = length(unique(id_all)),
#               min_train = min(id_all),
#               max_train = ceiling(nn*0.7)+min_train,
#               min_test = max_train + 1,
#               max_test = max(id_all))
# ind_training <- c(ind$min_train[1] : ind$max_train[1], ind$min_train[2] : ind$max_train[2])
# ind_test <- c(ind$min_test[1] : ind$max_test[1], ind$min_test[2] : ind$max_test[2])


training_df <- df_scaled[which(data1$id_all %in% ind_training),]
test_df <- df_scaled[which(data1$id_all %in% ind_test),]
```

```{r}

## Aus X2 LSTM exp.
# zuord0 <- 
#     data1 %>% 
#     select(id_all, satz_id, variante_H2O, wiederholung) %>% 
#     distinct() %>% 
#     mutate(training_test = ifelse(id_all %in% ind_training, 
#                                   "Training_df", "Test_df"))
# zuord0
```



#Sequence-Funktion 2
```{r}
## Diese Funktion ist auch gespeichert unter... Funktionen/to_sequence.R

to_sequence <- function(.data, spalte_y = "B0020_nFK_prozent", spalte_x = "T0", n_future = 1, n_past = 5){
    # n_future = 1   # Number of days we want to look into the future based on the past days.
    # n_past = 5  # Number of past days we want to use to predict the future.
    
    #Empty lists to be populated using formatted training data
    trainX <- matrix(ncol = length(spalte_x)) 
    trainY <- matrix(ncol = length(spalte_y)) 
    
    range1 <- range(n_past, nrow(.data) - n_future)
    seq1 <- seq(from=range1[1], to=range1[2])
    
    for (i in seq1){
        idx1 <- (1 + i - n_past):i
        trainX <- rbind(trainX, 
                        as.matrix(.data)[idx1, spalte_x, drop = FALSE]) 
        #5 aufeinander folgende Werte
        trainY <- rbind(trainY, 
                        as.matrix(.data)[(i + n_future), spalte_y, drop = FALSE]) 
        #nur eine Zahl
    }
    trainX <- trainX[-1, ,  drop = FALSE] #ersten NA-Eintrag löschen
    trainY <- trainY[-1, ,  drop = FALSE]
    
    return(list(x = trainX, y=trainY))
}

# # #Beispiel: Funktion anwenden:
# .data <- training_df[training_df[,"id_all"] == 1, c(names_y, names_x), drop = FALSE]
# 
# t <- to_sequence(.data = .data,
#                  spalte_y = names_y,
#                  spalte_x = c(names_y, names_x[1:2]),
#                  n_future = 1,
#                  n_past = 5)$x
# k <- keras::k_reshape(t, shape = c(9,5,3)) #c(9,5,21))
# 
# head(t)
# k
```



```{r}
n_past = 7

# Funktion anwenden: alle Abfolgen (14 Tage Intervall) in Vor-Tage-Matrix auflösen
training_sequence <- map(
    # 1:max
    unique(training_df[,"id_all"]),
    ~training_df[training_df[,"id_all"] == .x, , drop = FALSE] %>% 
        to_sequence(.data = .,
                    spalte_y = names_y, 
                    spalte_x = c(names_y, names_x), 
                    n_future = 1, 
                    n_past = n_past)
) 
train_x <- map(training_sequence, ~.x[["x"]]) %>% list_c()
train_y <- map(training_sequence, ~.x[["y"]]) %>% list_c()


test_sequence <- map(
    unique(test_df[,"id_all"]),
    ~test_df[test_df[,"id_all"] == .x, , drop = FALSE] %>% 
        to_sequence(.data = .,
                    spalte_y = names_y, 
                    spalte_x = c(names_y, names_x), 
                    n_future = 1, 
                    n_past = n_past)
) 
test_x <- map(test_sequence, ~.x[["x"]]) %>% list_c()
test_y <- map(test_sequence, ~.x[["y"]]) %>% list_c()


# cat("Head des Trainingsdatensatzes:\n")
# head(train_x)
# cat("\n\n")
# cat(paste("Der Trainingsdatensatz enthält *", NROW(train_x), "* Zeilen."))
```


# Reshape: # batch size, timesteps, features
```{r}
#Reformat input data into a shape: (n_samples x timesteps x n_features)
n_seq1 = length(training_sequence)
n_seq_test <- length(test_sequence)
rows1 <- kulturdauer - n_past #14-n_past 
ncol1 <- length(names_x) + length(names_y) #Spalten == NCOL(train_x) == 20 Features
ncol2 <- length(names_y)
### falsch:
###rows_total <- rows1 * n_seq1
###rows_total_test <- rows1 * n_seq_test
total_elements <- nrow(train_x) * ncol(train_x)
rows_total = total_elements / (n_past * ncol1)
total_elements_test <- nrow(test_x) * ncol(test_x)
rows_total_test <- total_elements_test / (n_past * ncol1)


train_x <- keras::k_reshape(train_x, shape = c(rows_total,n_past, ncol1)) 
train_y <- keras::k_reshape(train_y, shape = c(rows_total,ncol2)) 
test_x <-  keras::k_reshape(test_x, shape = c(rows_total_test,n_past, ncol1)) 
test_y <-  keras::k_reshape(test_y, shape = c(rows_total_test,ncol2)) 
```


#Länge des Tensors anpassen, für stateful = TRUE
```{r}
## train_x: shape=(909, 5, 22)

# train_x <- train_x[1:900,,]
# train_y <- train_y[1:900,]
```

# LSTM-Model initiieren:
```{r}
batch_size <- 32*4 #8 #*100 # 50 #4

lstm_model <- keras_model_sequential()

lstm_model %>%
    layer_lstm(units = 128, #64,
               activation = "sigmoid",
               batch_input_shape = c(batch_size, n_past, ncol1),
               return_sequences = FALSE, #nur bei mehreren Layern TRUE. Sonst FALSE
               stateful = F# TRUE
    ) %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = ncol2) # 1 (eine Schicht) oder 3 (alle Schichten)
```


# Kompilieren:
```{r}
lstm_model %>%
    compile(loss = 'mae', 
            ##optimizer = optimizer_rmsprop(learning_rate = 0.001),
            optimizer = 'rmsprop',
            #optimizer = optimizer_adam(learning_rate = 0.001),
            #optimizer = 'adam', 
            metrics = 'accuracy')

summary(lstm_model)
```

# Model trainieren:
```{r, message=FALSE}
earlystopping = callback_early_stopping(monitor = "loss", 
                                        min_delta = 0.001, #da standardisierte Daten
                                        patience = 5, 
                                        mode = "min", 
                                        restore_best_weights = TRUE                                            
)

lstm_model %>%          
    fit(train_x,train_y,
        epochs = 100,
        batch_size = batch_size,
        validation_split = 0.2, 
        shuffle = FALSE, 
        callbacks = earlystopping
    )
```


# Model evaluieren:
```{r}
#lstm_model %>% evaluate(test_x, test_y, batch_size = batch_size)
lstm_model %>% evaluate(train_x, train_y, batch_size = batch_size)
```

# make Predictions 
```{r}
#pred <- lstm_model %>% predict(test_x, batch_size = batch_size, stateful = TRUE) 
pred <- lstm_model %>% predict(train_x, batch_size = batch_size, stateful = TRUE) 

#rescale to normal nFK-values:
sy <- matrix(s[names_y], nrow = nrow(pred), ncol = length(names_y), byrow = T)
my <- matrix(m[names_y], nrow = nrow(pred), ncol = length(names_y), byrow = T)
pred_nFK <- pred[,1:ncol2] * sy + my

#test_y_nFK <- as.matrix(test_y) * sy + my
test_y_nFK <- as.matrix(train_y) * sy + my  
```

# Gütemaße:
```{r}
## Limitieren auf Wertebereich 0-100% nFK
id_100 <- which(test_y_nFK[,1] <= 100)

rmse1 <- round(colMeans(sqrt((test_y_nFK[id_100,]-pred_nFK[id_100,])^2)), 2)
rmse_text <- paste0( paste0("RMSE y", 1:ncol2, ": "), rmse1, collapse = "\n")
cat(rmse_text)
cat("\n\n")
lm1 <- map(1:ncol2, ~summary(lm(pred_nFK[id_100,.x]~test_y_nFK[id_100,.x])))


cat(map_chr(1:ncol2, ~paste0("R2 y",.x ," = ", round(lm1[[.x]]$r.squared, 3), "\n")))
```

# Ergebnis zu Tabelle zusammenführen:
```{r}
labels1 <- c("00-20 cm", 
             "20-40 cm", 
             "40-60 cm")

ergebnis_df_erstellen <- function(measured=test_y_nFK, predicted=pred_nFK, labels1=labels1
){
    x1 <- measured %>% as.data.frame() %>%
        tidyr::pivot_longer(cols = everything(), values_to = "measured", names_to = "depth")
    
    y1 <- predicted %>% as.data.frame() %>%
        tidyr::pivot_longer(cols = everything(), values_to = "predicted", names_to = "depth") %>% select(predicted)
    result1 <- bind_cols(x1, y1) %>%
        mutate(across("depth", ~factor(.,labels= labels1 ))) %>% 
        group_by(depth) %>% 
        mutate(x= 1:n() ,
               diff_meas_pred = abs(measured - predicted),
               diff_big = ifelse(diff_meas_pred > 5, predicted, NA))
    
    return(result1)
}

result <- ergebnis_df_erstellen(measured=test_y_nFK, predicted=pred_nFK, labels1=labels1)
```

# Ergebnis plotten:
```{r, warning=FALSE}
source("Functions/plot_measured_predicted_lm.R")

plot_measured_predicted_lm(linear_model = lm1, data = result, alpha = 0.01) +
    scale_x_continuous(limits = c(0,150)) +
    scale_y_continuous(limits = c(0,150))
```

# Modell speichern:
```{r, eval=TRUE}
filepath_tf <- "../data/derived_data/Models_paper/X5_Model3_2a_LSTM_pretrained_AMBAV"
# keras::save_model_tf(lstm_model, filepath = filepath_tf, overwrite = TRUE)
```