---
title: "ANNI in Tensorflow X6 Model MLP pretrained2 experimental data"
author: "Samantha Rubo"
date: "2023-08-10"
---

```{r, message=FALSE}
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
library(ggplot2)
library(data.table)
library(purrr)
library(tidyr)
```

### Daten prozessieren gleich zu X2 Model (Experimental Data, LSTM)


### Input-Tabelle laden (aus A3_Tabelle_erstellen.RMD)
Die Daten haben ein Intervall von 14 Tagen

```{r}
#rm(list = ls())
files2 <- c(
    # alt = "../data/derived_data/input_tabelle_2020_2023_20230328_20cm_complete.csv"
    # "../data/derived_data/input_tabelle_2020_2023_20240328_20cm_complete.csv",
    # "../data/derived_data/input_tabelle_2020_2023_20240328_20cm_complete_DLR.csv"
    HGU =  "../data/derived_data/input_tabelle_2020_2023_20240403_lstm_geeignet_20cm_complete.csv",
    DLR =  "../data/derived_data/input_tabelle_2020_2023_20240403_lstm_geeignet_20cm_complete_DLR.csv"
)

data0 <- map_df(files2, 
                ~data.table::fread(.x, sep = ";", dec = "."), .id = "Ort"
) %>% as_tibble() %>%
    group_by(Ort, satz_id, variante_H2O, wiederholung) %>%
    mutate(id_all = cur_group_id() )%>%#, .after = "id") %>%
    # select(-id) %>%
    ungroup() %>%
    arrange(id_all, tage_seit_aussaat)

columns_with_nFK <- grep("nFK", names(data0), value = TRUE)

# Step 2: Filter rows where all 'nFK' columns have values <= 100 /// Nicht bei LSTM
# data0 <- data0 %>%
#     #filter(across(all_of(columns_with_nFK), ~ . <= 100)) #bei MLP
#     mutate(across(all_of(columns_with_nFK), ~ifelse(.>150,150,.))) #Bei LSTM

names(data0)

kulturdauer = 14
```

# X und Y Parameter benennen:
```{r}
names_x <- c("tage_seit_aussaat", 
             "Tmean_gradC", 
             "Tmean_th9_25_gradC", 
             "Tmin_gradC", 
             "Tmax_gradC", 
             "relLuftfeuchte_mean_prozent", 
             "relLuftfeuchte_min_prozent", 
             "relLuftfeuchte_max_prozent", 
             "globalstrahlung_Wh_m2", 
             "windgeschwindigkeit_m_s", 
             "tageslicht_h", 
             "tageslicht_h_th9", 
             "Tmean_gradC_sum", 
             "Tmean_th9_25_gradC_sum", 
             "globalstrahlung_Wh_m2_sum", 
             "niederschlag_mm_sum", 
             "tageslicht_h_sum", 
             "tageslicht_h_th9_sum", 
             "Ton_prozent",
             "Schluff_prozent",
             "Sand_prozent",
             "C_org_prozent",
             "ibi", 
             "irmi",
             "wasser_input"
)

names_y <- c(
    "B0020_nFK_prozent", 
    "B2040_nFK_prozent",
    "B4060_nFK_prozent"
)
data1 <- data0 %>% select(all_of(c(names_y,names_x, "id_all"))) 

```

# Parameter auswählen
```{r}
data1 <- data0 %>% 
    select(all_of(c("id_all", names_y, names_x))) %>%
    mutate_all(as.numeric) %>% as.matrix()
```


# Plot einiger Zeitreihen
```{r}
df_plot <- data0 %>% group_by(id_all) %>% filter(id_all < 50) %>% mutate(Tage = 1:n()) 

ggplot(df_plot, aes(Tage, B2040_nFK_prozent, color = as.factor(id_all))) + 
    geom_line() + 
    theme_bw() + 
    theme(legend.position = "none", panel.grid = element_blank())
```




#Daten skalieren
Später neue Daten anhand dieser Werte skalieren

## Skalierungs-Daten einlesen:
```{r}
scaling_data <- fread("../data/derived_data/Models_paper/All_scaling_data_20230328.csv")

names_all <- scaling_data$parameter
m <- scaling_data$m
s <- scaling_data$s

names(m) <- names_all
names(s) <- names_all
```

## DF skalieren
```{r}
df_scaled <- scale(data1[,c(names_y, names_x,"id_all")], 
                   center = m[c(names_y, names_x,NA)], 
                   scale = s[c(names_y, names_x,NA)])

df_scaled[,"id_all"] <- data0$id_all
```


# Aufteilung in Trainings- und Test-Datensatz
```{r}
# In Training und Test-Daten einteilen:

ids <- unique(data0$id_all)

## alt, bei nur HGU-Daten:
# ind <- ceiling(length(ids) * 0.7)
# ind_training <- ids[1:ind] 
# ind_test <- ids[(ind+1):length(ids)]

#Ids nach Ort aufteilen:
ind <- data0 %>%
    group_by(Ort) %>% 
    summarise(nn = length(unique(id_all)),
              min_train = min(id_all),
              max_train = ceiling(nn*0.7)+min_train,
              min_test = max_train + 1,
              max_test = max(id_all))

ind_training <- c(ind$min_train[1] : ind$max_train[1], ind$min_train[2] : ind$max_train[2])
ind_test <- c(ind$min_test[1] : ind$max_test[1], ind$min_test[2] : ind$max_test[2])


training_df <- df_scaled[which(data0$id_all %in% ind_training),]
test_df <- df_scaled[which(data0$id_all %in% ind_test),]
```

```{r}
zuord0 <- 
    data0 %>% 
    select(id_all, satz_id, variante_H2O, wiederholung) %>% 
    distinct() %>% 
    mutate(training_test = ifelse(id_all %in% ind_training, 
                                  "Training_df", "Test_df"))
zuord0
```



#Sequence-Funktion 2
```{r}
## Diese Funktion ist auch gespeichert unter... Funktionen/to_sequence.R

to_sequence <- function(.data, spalte_y = "B0020_nFK_prozent", spalte_x = "T0", n_future = 1, n_past = 5){
    # n_future = 1   # Number of days we want to look into the future based on the past days.
    # n_past = 5  # Number of past days we want to use to predict the future.
    
    #Empty lists to be populated using formatted training data
    trainX <- matrix(ncol = length(spalte_x)) 
    trainY <- matrix(ncol = length(spalte_y)) 
    
    range1 <- range(n_past, nrow(.data) - n_future)
    seq1 <- seq(from=range1[1], to=range1[2])
    
    for (i in seq1){
        idx1 <- (1 + i - n_past):i
        trainX <- rbind(trainX, 
                        as.matrix(.data)[idx1, spalte_x, drop = FALSE]) 
        #5 aufeinander folgende Werte
        trainY <- rbind(trainY, 
                        as.matrix(.data)[(i + n_future), spalte_y, drop = FALSE]) 
        #nur eine Zahl
    }
    trainX <- trainX[-1, ,  drop = FALSE] #ersten NA-Eintrag löschen
    trainY <- trainY[-1, ,  drop = FALSE]
    
    return(list(x = trainX, y=trainY))
}

# # #Beispiel: Funktion anwenden:
# .data <- training_df[training_df[,"id_all"] == 1, c(names_y, names_x), drop = FALSE]
# 
# t <- to_sequence(.data = .data,
#                  spalte_y = names_y,
#                  spalte_x = c(names_y, names_x[1:2]),
#                  n_future = 1,
#                  n_past = 5)$x
# k <- keras::k_reshape(t, shape = c(9,5,3)) #c(9,5,21))
# 
# head(t)
# k
```



```{r}
n_past = 7

# Funktion anwenden: alle Abfolgen (14 Tage Intervall) in Vor-Tage-Matrix auflösen
training_sequence <- map(
    # 1:max
    unique(training_df[,"id_all"]),
    ~training_df[training_df[,"id_all"] == .x, , drop = FALSE] %>% 
        to_sequence(.data = .,
                    spalte_y = names_y, 
                    spalte_x = c(names_y, names_x), 
                    n_future = 1, 
                    n_past = n_past)
) 
train_x <- map(training_sequence, ~.x[["x"]]) %>% list_c()
train_y <- map(training_sequence, ~.x[["y"]]) %>% list_c()


test_sequence <- map(
    unique(test_df[,"id_all"]),
    ~test_df[test_df[,"id_all"] == .x, , drop = FALSE] %>% 
        to_sequence(.data = .,
                    spalte_y = names_y, 
                    spalte_x = c(names_y, names_x), 
                    n_future = 1, 
                    n_past = n_past)
) 
test_x <- map(test_sequence, ~.x[["x"]]) %>% list_c()
test_y <- map(test_sequence, ~.x[["y"]]) %>% list_c()


# cat("Head des Trainingsdatensatzes:\n")
# head(train_x)
# cat("\n\n")
# cat(paste("Der Trainingsdatensatz enthält *", NROW(train_x), "* Zeilen."))
```


# Reshape: # batch size, timesteps, features
```{r}
#Reformat input data into a shape: (n_samples x timesteps x n_features)
n_seq1 = length(training_sequence)
n_seq_test <- length(test_sequence)
rows1 <- kulturdauer - n_past #14-n_past 
ncol1 <- length(names_x) + length(names_y) #Spalten == NCOL(train_x) == 20 Features
ncol2 <- length(names_y)
### falsch:
###rows_total <- rows1 * n_seq1
###rows_total_test <- rows1 * n_seq_test
total_elements <- nrow(train_x) * ncol(train_x)
rows_total = total_elements / (n_past * ncol1)
total_elements_test <- nrow(test_x) * ncol(test_x)
rows_total_test <- total_elements_test / (n_past * ncol1)


train_x <- keras::k_reshape(train_x, shape = c(rows_total,n_past, ncol1)) 
train_y <- keras::k_reshape(train_y, shape = c(rows_total,ncol2)) 
test_x <-  keras::k_reshape(test_x, shape = c(rows_total_test,n_past, ncol1)) 
test_y <-  keras::k_reshape(test_y, shape = c(rows_total_test,ncol2)) 
```






<!-- #### TRANSFER LEARNING -->

<!-- ### Gespiechertes Modell X3 laden: -->
<!-- ```{r, eval=TRUE} -->
<!-- rm(model, new_model) -->
<!-- filepath_tf <- "../data/derived_data/Models_paper/X5_Model3_2a_LSTM_pretrained_AMBAV/" -->
<!-- new_model <- load_model_tf(filepath_tf) ### direkt als new_model einlesen! -->

<!-- # Check its architecture -->
<!-- summary(new_model) -->
<!-- # fs::dir_tree(filepath_tf) #Ordnerstruktur anzeigen lassen: -->
<!-- ``` -->

<!-- # Weights extrahieren und auf neues erweitertes Modell legen -->
<!-- ```{r} -->
<!-- # old_input_layer = get_layer(model,index = 1) -->
<!-- # nn = ncol(training_df) -->
<!-- # new_input_layer = layer_input( -->
<!-- #     shape=c(nn), -->
<!-- #     dtype=old_input_layer$dtype, -->
<!-- #     name='test' -->
<!-- # ) -->
<!-- #  -->
<!-- # new_model = clone_model(model, new_input_layer) -->
<!-- #  -->
<!-- # ## weights müssen noch kopiert werden. -->
<!-- # for (i in 2:length(new_model$layers)){ -->
<!-- #     set_weights(new_model$layers[[i]], get_weights(model$layers[[i]])) -->
<!-- # } -->
<!-- # summary(new_model) -->
<!-- #  -->
<!-- # rm(model) -->
<!-- ``` -->


### Neu trainieren mit Exp Daten
```{r}
batch_size <- 16
```

```{r}
earlystopping = callback_early_stopping(monitor = "loss",
                                        min_delta = 0.001, #da auch Y skaliert ist
                                        patience = 5,
                                        mode = "min",
                                        restore_best_weights = TRUE
)
new_model %>% compile(
    #optimizer = optimizer_adam(learning_rate = 0.0001),#'rmsprop',#"Adam",#'rmsprop',
    optimizer = optimizer_rmsprop(learning_rate = 0.0001),
    loss = 'mse',
    metrics = c('mae', 'accuracy')
)

new_model %>%
    fit(train_x,train_y,
        epochs = 100,
        batch_size = batch_size,
        validation_split = 0.1,
        shuffle = FALSE, 
        callbacks = earlystopping
    )
```

learning rate anpassen!

Small Learning Rate (e.g., 0.0001):
Preferred for Fine-Tuning: A smaller learning rate is generally preferred when you're fine-tuning a pre-trained model. It ensures that the model's weights are adjusted slowly and carefully, preventing significant deviations from the learned representations that have already proven to be effective on the original dataset. This is particularly important when the new dataset is relatively small or very similar to the original dataset, as drastic changes could lead to overfitting or forgetting valuable learned features.
Gradual Adjustments: It allows the model to make gradual adjustments, integrating the new information from the experimental data without drastically altering the effective patterns and structures learned from the previous training.


# Model evaluieren:
```{r}
#lstm_model %>% evaluate(test_x, test_y, batch_size = batch_size)
new_model %>% evaluate(train_x, train_y, batch_size = batch_size)
```

# make Predictions 
```{r}
#pred <- new_model %>% predict(test_x, batch_size = batch_size, stateful = TRUE) 
pred <- new_model %>% predict(train_x, batch_size = batch_size, stateful = TRUE) 

#rescale to normal nFK-values:
sy <- matrix(s[names_y], nrow = nrow(pred), ncol = length(names_y), byrow = T)
my <- matrix(m[names_y], nrow = nrow(pred), ncol = length(names_y), byrow = T)
pred_nFK <- pred[,1:ncol2] * sy + my

#test_y_nFK <- as.matrix(test_y) * sy + my
test_y_nFK <- as.matrix(train_y) * sy + my  
```

# Gütemaße:
```{r}
## Limitieren auf Wertebereich 0-100% nFK
id_100 <- which(test_y_nFK[,1] <= 100)

rmse1 <- round(colMeans(sqrt((test_y_nFK[id_100,]-pred_nFK[id_100,])^2)), 2)
rmse_text <- paste0( paste0("RMSE y", 1:ncol2, ": "), rmse1, collapse = "\n")
cat(rmse_text)
cat("\n\n")
lm1 <- map(1:ncol2, ~summary(lm(pred_nFK[id_100,.x]~test_y_nFK[id_100,.x])))


cat(map_chr(1:ncol2, ~paste0("R2 y",.x ," = ", round(lm1[[.x]]$r.squared, 3), "\n")))
```

# Ergebnis zu Tabelle zusammenführen:
```{r}
labels1 <- c("00-20 cm", 
             "20-40 cm", 
             "40-60 cm")

ergebnis_df_erstellen <- function(measured=test_y_nFK, predicted=pred_nFK, labels1=labels1
){
    x1 <- measured %>% as.data.frame() %>%
        tidyr::pivot_longer(cols = everything(), values_to = "measured", names_to = "depth")
    
    y1 <- predicted %>% as.data.frame() %>%
        tidyr::pivot_longer(cols = everything(), values_to = "predicted", names_to = "depth") %>% select(predicted)
    result1 <- bind_cols(x1, y1) %>%
        mutate(across("depth", ~factor(.,labels= labels1 ))) %>% 
        group_by(depth) %>% 
        mutate(x= 1:n() ,
               diff_meas_pred = abs(measured - predicted),
               diff_big = ifelse(diff_meas_pred > 5, predicted, NA))
    
    return(result1)
}

result <- ergebnis_df_erstellen(measured=test_y_nFK, predicted=pred_nFK, labels1=labels1)
```

# Ergebnis plotten:
```{r, warning=FALSE}
source("Functions/plot_measured_predicted_lm.R")

plot_measured_predicted_lm(linear_model = lm1, data = result) +
    scale_x_continuous(limits = c(0,150)) +
    scale_y_continuous(limits = c(0,150))
```

#adaptiertes Modell speichern:
```{r, eval=TRUE}
filepath_tf <- "../data/derived_data/Models_paper/X6_Model3_2b_LSTM_pretrained2_experimental_data/"
# keras::save_model_tf(new_model, filepath = filepath_tf, overwrite = TRUE)
```


