---
title: "ANNI_0 erstellen"
author: "Samantha Rubo"
date: '2021-12-15'
output: html_notebook
---
```{r}
rm(list = ls())
```
\
### Bibliotheken laden
```{r message=FALSE, warning=FALSE, include=FALSE}
library(data.table)
library(dplyr)
library(ggplot2)
library(lubridate)
#library(ggnewscale)
#library(tidyr)
library(neuralnet)
#library(keras)
```
\
### Input-Tabelle laden (aus A3_Tabelle_erstellen.RMD)
```{r}
input_tabelle <- data.table::fread(
    "../data/derived_data/input_tabelle_2021_Satz1_b.csv", 
    sep = ";", dec = ".") %>%
    mutate_at("Datum", ~as_date(.))
input_tabelle <- data.table::setDT(input_tabelle)
```
\
### nFK vom Vortag als Input ausweisen:
```{r}
input_tabelle <- 
    input_tabelle %>%
    mutate(T0020_nFK_Vortag = c(NA,T0020_nFK[1:(n()-1)]),
           T2040_nFK_Vortag = c(NA,T2040_nFK[1:(n()-1)]),
           T4060_nFK_Vortag = c(NA,T4060_nFK[1:(n()-1)])) %>%
    slice(-1) #erste Zeile entfernt, da hier NA
```
\
### Wassereintrag aus P + Bewässerung berechnen. Benötigte Variablen selektieren.
```{r}
input_df <- input_tabelle %>%
    select(T0060_mm, T0020_nFK_Vortag,T2040_nFK_Vortag, T4060_nFK_Vortag,Tage_seit_Aussaat,Tmax,Tmin,Tmean,RF,Glob,RHmin,V2,P,bewaesserung_mm) %>%
    rowwise() %>%
    mutate(wasser_input = sum(P, bewaesserung_mm, na.rm = TRUE)) %>%
    ungroup %>%
    mutate(wasser_input_Vortag = c(NA,wasser_input[1:(n()-1)])) %>%
    select(-P, -bewaesserung_mm, -wasser_input) 

```
\
### Datensatz skalieren --> notwendig als Input für ANN
```{r}
# The predictor vars must be scaled data for the ANN fitting
input_df.scaled <- data.table::as.data.table(scale(input_df))
min.T0060_mm <- min(input_df$T0060_mm, na.rm = TRUE)
max.T0060_mm <- max(input_df$T0060_mm, na.rm = TRUE)
# response var must be scaled to [0 < resp < 1]
input_df.scaled$T0060_mm <- scale(input_df$T0060_mm, 
                                  center = min.T0060_mm, 
                                  scale = max.T0060_mm - min.T0060_mm)

# Train-test split
```
\
### Index für Trainings- und Test-Datensatz
```{r}
set.seed(123)
idx <- sample(1:nrow(input_df.scaled), size = tail(nrow(input_df.scaled)*0.7) )
train <- input_df.scaled[idx,]
test <- input_df.scaled[-idx,]
```
\
### ANNI erstellen
```{r}
# Custom activation function
#softplus <- function(x) log(1 + exp(x))

nn=neuralnet(T0060_mm~Tage_seit_Aussaat+T0020_nFK_Vortag+T2040_nFK_Vortag+ T4060_nFK_Vortag+Tmean+RF+Glob+RHmin+V2+wasser_input_Vortag,
             data=train,
             hidden=10,
             act.fct = "logistic", #softplus
             linear.output = FALSE,
             algorithm = "backprop", #backpropagation
             learningrate = 0.0001, 
             #threshold=0.01,
             #rep=5,
             stepmax = 1e+06) #1000000
```
\
### Nodes plotten
```{r}
#plot(nn)
plot(nn,col.hidden = 'darkgreen', 
     col.hidden.synapse = 'darkgreen',
     show.weights = F,
     information = F,
     fill = 'lightblue')
```
\
### Modell mit Test-Daten evaluieren
```{r}
Predict <- compute(nn,test)
#Predict$net.result
```

```{r}
# prob <- Predict$net.result
# pred <- ifelse(prob>0.5, 1, 0)
# pred
```

```{r}
#NeuralNetTools::garson(nn)
```

```{r}
# Nur bei einem hidden layer möglich.
#NeuralNetTools::lekprofile(nn)
```
\
### MSE berechnen:
```{r}
y_raw <- input_df$T0060_mm
maxy <-max(y_raw)
miny <-min(y_raw)

Predict_ <- Predict$net.result*(maxy-miny)+miny

test.r <- (test$T0060_mm)*(maxy-miny)+ miny

MSE.nn <- sum((test.r - Predict_)^2, na.rm = TRUE)/nrow(test)
MSE.nn
```
\


```{r}
predict_all <- compute(nn, input_df.scaled)
predict_all_net <- predict_all$net.result*(maxy-miny)+miny
#predict_all_net
```

### Plot: Predicted values ~ actual values
```{r}
#rm(df1, df2)
df1 <- as.data.table(data.frame(actual_value = y_raw, 
                               predicted_value = predict_all_net,
                               input = "alle_Daten"))
df2 <- as.data.table(data.frame(actual_value =test.r, 
                               predicted_value = Predict_,
                               input = "test_Daten"))

ggplot(bind_rows(df1, df2), aes(actual_value, predicted_value)) + 
    geom_point() + 
    geom_abline(slope = 1, intercept = 0, color = "red")+
    ylab("ANN predicted AWC (%)") + xlab("actual AWC (%)") + 
    facet_grid(.~input) + 
    theme_bw() + 
    coord_fixed() + 
    theme(panel.grid = element_blank())
```





# Keras Tensorflow

<!-- #V2 -->
<!-- ```{r} -->
<!-- n <- neuralnet(T0060_mm~Tage_seit_Aussaat+T0020_nFK_Vortag+T2040_nFK_Vortag+ T4060_nFK_Vortag+                 Tmax+Tmin+Tmean+RF+Glob+RHmin+V2+wasser_input, -->
<!--                data=input_df[idx,], -->
<!--                hidden = c(12,7), -->
<!--                linear.output = F, -->
<!--                lifesign = 'full', -->
<!--                rep=1) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- plot(n) -->
<!-- ``` -->


<!-- #V3 -->

<!-- ```{r} -->
<!-- data <- as.matrix(input_df) -->
<!-- dimnames(data) <- NULL -->
<!-- ``` -->

<!-- ```{r} -->
<!-- set.seed(123) -->
<!-- ind <- sample(2, nrow(data), replace = T, prob = c(.7, .3)) -->
<!-- training <- data[ind==1,2:13] -->
<!-- test <- data[ind==2, 2:13] -->
<!-- trainingtarget <- data[ind==1, 1] -->
<!-- testtarget <- data[ind==2,1] -->
<!-- str(trainingtarget) -->
<!-- str(testtarget) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- m <- colMeans(training) -->
<!-- s <- apply(training, 2, sd) -->
<!-- training <- scale(training, center = m, scale = s) -->
<!-- test <- scale(test, center = m, scale = s) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- model <- keras::keras_model_sequential() -->
<!-- model %>% -->
<!--     layer_dense(units = 5, activation = 'relu', input_shape = c(12)) %>% -->
<!--     layer_dense(units = 1) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- model %>% compile(loss = 'mse', -->
<!--                   optimizer = 'rmsprop',  -->
<!--                   metrics = 'mae')  -->
<!-- ``` -->

<!-- ```{r} -->
<!-- mymodel <- model %>%           -->
<!--     fit(training,trainingtarget, -->
<!--         epochs = 100, -->
<!--         batch_size = 32, -->
<!--         validation_split = 0.2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- model %>% evaluate(test, testtarget) -->
<!-- pred <- model %>% predict(test) -->
<!-- mean((testtarget-pred)^2)  -->
<!-- ``` -->
<!-- ```{r} -->
<!-- plot(testtarget, pred)  -->
<!-- ``` -->







<!-- ```{r} -->
<!-- model <- keras::keras_model_sequential() -->

<!-- model %>% -->
<!--     layer_dense(units = 20, activation = 'relu', input_shape = c(12)) %>% -->
<!--     layer_dropout(rate=0.4)  %>% -->
<!--     layer_dense(units = 50, activation = 'relu')  %>% -->
<!--     layer_dropout(rate=0.2)  %>% -->
<!--     layer_dense(units = 20, activation = 'relu')  %>% -->
<!--     layer_dropout(rate=0.2)  %>% -->
<!--     layer_dense(units = 1) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- model %>% compile(loss = 'mse', -->
<!--                   optimizer = 'rmsprop',  -->
<!--                   metrics = 'mae')  -->
<!-- ``` -->

<!-- ```{r message=FALSE, include=FALSE} -->
<!-- mymodel <- model %>%           -->
<!--     fit(training,trainingtarget, -->
<!--         epochs = 100, -->
<!--         batch_size = 32, -->
<!--         validation_split = 0.2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- model %>% evaluate(test, testtarget) -->
<!-- pred <- model %>% predict(test) -->
<!-- mean((testtarget-pred)^2)  -->
<!-- ``` -->
<!-- ```{r} -->
<!-- plot(testtarget, pred)  -->
<!-- ``` -->



<!-- #V4 -->
<!-- ```{r} -->

<!-- ``` -->

