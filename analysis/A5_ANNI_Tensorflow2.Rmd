---
title: "ANNI in Tensorflow"
author: "Samantha Rubo"
date: "2022-08-10"
---


```{r}
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
library(ggplot2)
```


### Input-Tabelle laden (aus A3_Tabelle_erstellen.RMD)
```{r}
rm(list = ls())
data0 <- data.table::fread(
    "../data/derived_data/input_tabelle_2020_2022_20220808_complete.csv", 
    sep = ";", dec = ".") %>% as_tibble()

```
\
#Variablen ausw채hlen:
```{r}
# data <- data0 %>%
#     mutate(nFK_0010_change = nFK_0010_Vortag - nFK_0010,
#            nFK_1020_change = nFK_1020_Vortag - nFK_1020,
#            nFK_2030_change = nFK_2030_Vortag - nFK_2030,
#            nFK_3040_change = nFK_3040_Vortag - nFK_3040,
#            nFK_4050_change = nFK_4050_Vortag - nFK_4050,
#            nFK_5060_change = nFK_5060_Vortag - nFK_5060, .before = "nFK_0010"
#            ) %>%
#     select(!any_of(c("nFK_0010", "nFK_1020", "nFK_2030", 
#                      "nFK_3040", "nFK_4050", "nFK_5060"))) %>%
#     select(!contains("sum"))
```

#ANNI: 
#Matrix mit y (Bodenfeuchte pro Schicht: "nFK_0010" bis "nFK_5060)
#und x (Wetterdaten, Bodenfeuchte pro Schicht des Vortages, Tage seit Aussaat) 
Tage seit Aussaat 채ndern in Tage seit Auflaufen??
```{r}
data <- data0 %>% mutate_all(as.numeric)
data <- as.matrix(data)
dimnames(data) <- NULL
set.seed(123)
ind <- sample(2, nrow(data), replace = T, prob = c(.7, .3))

#X definieren:
#Ab Spalte 7
idx_x <- 7:ncol(data)
training <- data[ind==1,idx_x]
test <- data[ind==2, idx_x]

#Target definieren:
#erste 6 Spalten sind Y-Werte (nFK 0 - 60 cm)
idx_y <- 1:6
trainingtarget <- data[ind==1, idx_y] 
testtarget <- data[ind==2, idx_y]

str(trainingtarget)
str(testtarget)
```


#Daten skalieren. 
Sp채ter neue Daten anhand dieser Werte skalieren
```{r}
m <- colMeans(training)
s <- apply(training, 2, sd, na.rm=TRUE)
s <- ifelse(s == 0, 1, s)
training <- scale(training, center = m, scale = s)
test <- scale(test, center = m, scale = s)
```

#Training mit random NA values:
```{r}
# idx_na_ibi <- sample(1:nrow(training), size = 100, replace = FALSE)
# training[idx_na_ibi,29:30] <- NA
```

#Keras initiieren: Input-Layer und Output-Layer definieren
#A ) Sequential Model
```{r, eval=FALSE}
model <- keras_model_sequential()

#Modell weiter anpassen:
#Lernrate 채ndern, etc.
model %>%
    layer_dense(units = 100, activation = 'sigmoid', input_shape = c(length(idx_x))) %>%
    layer_dropout(rate=0.4)  %>%
    layer_dense(units = 50, activation = 'sigmoid')  %>%
    layer_dropout(rate=0.2)  %>%
    layer_dense(units = length(idx_y))

#Loss-Function beschreiben
model %>% compile(loss = 'mse',
                  optimizer = 'rmsprop', 
                  metrics = c('accuracy','mae')) 
#metrics = c('mae'))

# in Python:
# model.compile(
# optimizer=tf.optimizers.Adam(learning_rate=0.1),
# loss='mean_absolute_error' #squared
# )

# model.compile(optimizer='RMSprop',
# loss='mse',
# metrics=['mae'])
```

#B) Functional Model:
```{r}
inputs <- layer_input(shape = c(length(idx_x)))

# outputs compose input + dense layers
predictions <- inputs %>%
    layer_dense(units = 100, activation = 'sigmoid') %>%
    layer_dropout(rate=0.05)  %>%
    layer_dense(units = 50, activation = 'sigmoid') %>%
    # layer_dropout(rate=0.1)  %>%
    # layer_dense(units = 50, activation = 'sigmoid') %>%
    layer_dropout(rate=0.05)  %>%
    layer_dense(units = length(idx_y))

# create model
model <- keras_model(inputs = inputs, outputs = predictions)

# compile model
model %>% compile(
    optimizer = 'rmsprop',
    loss = 'mse',
    #    metrics = c('mae', 'accuracy')
    metrics = c('mae')
)

earlystopping = callback_early_stopping(monitor = "loss", 
                                        min_delta = 1, 
                                        patience = 5, 
                                        mode = "min", 
                                        restore_best_weights = TRUE                                            
)
```


#Model trainieren:
```{r, message=FALSE}
model %>%          
    fit(training,trainingtarget, 
        epochs = 256,
        batch_size = 32,
        validation_split = 0.2, 
        callbacks = earlystopping#,
        #verbose = FALSE
    )
```

#Model evaluieren:
```{r}
model %>% evaluate(test, testtarget)
pred <- model %>% predict(test)
mean((testtarget-pred)^2) 
```

#Ergebnis plotten:
```{r}
x1 <- testtarget %>% as.data.frame() %>% 
    tidyr::pivot_longer(cols = everything(), values_to = "measured", names_to = "depth")

y1 <- pred %>% as.data.frame() %>% 
    tidyr::pivot_longer(cols = everything(), values_to = "predicted") %>% select(predicted)

result <- bind_cols(x1, y1) %>% 
    mutate_at("depth", 
              ~factor(., labels = c("nFK_0010_change", "nFK_1020_change", "nFK_2030_change", 
                                    "nFK_3040_change", "nFK_4050_change", "nFK_5060_change") ))


ggplot(result, aes(measured, predicted)) + 
    geom_point() + 
    geom_abline(slope = 1, intercept = 0, color = "red", linetype = 2) + 
    facet_wrap(depth~.) + 
    theme_bw() + 
    theme(panel.grid = element_blank())
```
#Modell speichern:
```{r, eval=FALSE}
filepath_tf <- "../data/derived_data/Model1/ANNI_Tensorflow_20220819"
keras::save_model_tf(model, filepath = filepath_tf, overwrite = FALSE)
```

#Gespiechertes Modell laden:
```{r, eval=FALSE}
filepath_tf <- "../data/derived_data/Model1/ANNI_Tensorflow_20220819"
new_model <- load_model_tf(filepath_tf)

# Check its architecture
#summary(new_model)

#fs::dir_tree(filepath_tf) #Ordnerstruktur anzeigen lassen:

# Anwendung des gelandeden Modells wie oben:
# new_model %>% evaluate(test, testtarget)
# pred <- new_model %>% predict(test)
# mean((testtarget-pred)^2) 
```

